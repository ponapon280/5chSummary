以下は、提供された5chログ（644〜844）から、指定された生成AIの「モデル」に関する話題をすべて抽出したものです。抽出の基準は以下の通りです：

- 対象モデル：NovelAI v4もしくはv3 (NAI), Pony, illustrious 0.1,1.0,1.1,2,3,3.5vpred (イラストリアス, リアス,ill), Noobai, FLUX , HiDream, Wan2.1 (wan), FramePack
- ログ内の関連するレスをすべて抽出。特に、そのモデルが選ばれている理由（例: 精度、VRAM効率、生成速度など）が明記されている場合、それを強調して記載。
- 抽出はログのレス番号順に整理。重複や文脈のつながりを考慮し、関連するレスをまとめて記述。
- リスト外のモデル（例: Qwen-Imageなど）は抽出対象外とし、無視。
- 該当する話題がなかったモデルについては「該当なし」と明記。

### NovelAI v4もしくはv3 (NAI)
- 該当なし。

### Pony
- 該当なし。

### illustrious 0.1,1.0,1.1,2,3,3.5vpred (イラストリアス, リアス,ill)
- 713: Illustrious用のアイスクリーム機から直食いLoRAが来たか…
  - 内容: Illustriousモデル用のLoRA（アイスクリーム機から直食い関連）が登場した話題。選ばれている理由は明記なし（LoRAの新規性のみ）。

### Noobai
- 該当なし。

### FLUX , HiDream
- 803: 画像生成AI「Qwen-Image」登場、OpenAIやFlux超えの高品質画像を生成可能 gigazine.net/news/20250805-qwen-image-image-generation-ai/ なんか来た WANでおなじみAlibaba製
  - 内容: FluxがQwen-Imageと比較され、高品質画像生成で超えられているとされる話題。選ばれている理由は明記なし（比較のみ）。
- 827: Qwen 優秀でもFluxみたいにエロダメなら死産なんだよなワイの中で 求められるスペックも高いから個人で追加学習するのもきついか？
  - 内容: Fluxがエロ生成に不向き（エロダメ）であるため、ユーザーにとって死産扱いという話題。選ばれている理由は明記なし（逆に不向きな点が挙げられている）。
- HiDreamについては該当なし。

### Wan2.1 (wan)
- 658: ニキらはWAN22でモデルって何つことるんや？ easyのデフォのはGGUFのQ3やけど動くならやっぱQ8とかのがええんか？
  - 内容: Wan2.2（Wan2.1の後継版）で使用するモデルについて、EasyのデフォルトGGUF Q3とQ8の比較。選ばれている理由: Q8の方が良いか？という疑問（精度や動作性）。
- 664: >>658 wan2.1時代から合わせて2.2でも14B_Q4_K_MのGGUFを使ってる ちなみにLLMだと4bit量子化（Q4)までは精度低下が少ないと言われた 自分の環境に合わせてVRAMオーバーしないのを選ぶしかない
  - 内容: Wan2.1時代からWan2.2でも14B_Q4_K_MのGGUFを使用。選ばれている理由: 精度低下が少ない（4bit量子化Q4まで）、VRAMオーバーしないよう環境に合わせるため。
- 665: 3060でもQ8動かせとるからあとは時間と相談やな 電力2割減で回して640p81fが4～6分くらいやわ
  - 内容: Q8モデルが3060 GPUで動作可能。選ばれている理由: 時間と相談（生成速度）、電力削減で効率的に回せるため。
- 692: Wanのt2iで俺の中のアリババの信頼度は爆上がりだからQwen-imageには絶大な期待を寄せてるわ
  - 内容: WanのT2I（Text-to-Image）機能が信頼度を上げている。選ばれている理由: アリババ製の信頼性が高いため（爆上がり）。
- 694: wan2.2 14B FP8がhighとlowで28GBだけど オフロードすればミドルGPUでも行けるしばっちこいや qwen-imgの元のBF16が41.86GBだからFP8だとパラメータ数と同じ20GB位かな 量子化RTAまだかー
  - 内容: Wan2.2の14B FP8モデルが28GB使用。選ばれている理由: オフロードでミドルGPUでも動作可能、量子化でVRAMを抑えられるため。
- 696: 今は Wan 2.2ってのがあついんか 画像生成AIが出来るPCなら問題なく動画生成可能になってるのもすごいの 久しぶりにきたあかちゃんなんでびっくりや としあきwikiのSDXLモデルはあんま更新されてないけど 上位的な存在がでたか動画関連の方が今は主流なんかな
  - 内容: Wan 2.2が今熱いモデルとして話題。選ばれている理由: 画像生成PCで動画生成が可能（すごい）、動画関連が主流のため。
- 700: SDXLが動作する環境から多少メインメモリを盛るなど+α程度のマシンスペックで、低解像度設定なら余裕で動くのがwanの凄いとこやね
  - 内容: Wanの動作環境について。選ばれている理由: SDXL環境から少しメモリを増やせば低解像度で余裕で動く（凄いところ）。
- 712: メモリ32GBやから動画は諦めてたけどEasyWan2.2ダメ元でやってみたら普通に動いたわ… オリキャラに腰振りダンスさせるだけでめっちゃ楽しい Zuntanニキまじでありがとう
  - 内容: EasyWan2.2が32GBメモリで動作。選ばれている理由: 動画生成が諦めていたが普通に動く（楽しいため）。
- 715: SandyおじさんのメインメモリDDR3の32GBなマシンでもいけるんで VRAMも解像度640以下ならたぶん8GBでも足りてそうな感じ…？
  - 内容: WanがDDR3 32GBマシンで動作。選ばれている理由: 低スペック（解像度640以下、VRAM8GB）でも足りるため。
- 717: >>715 8GBならggufのQ4_K_SをSageAttetionとlightx2v噛ませて480x480の81フレーム行けると思う
  - 内容: VRAM8GBでGGUF Q4_K_Sを使用。選ばれている理由: SageAttentionとlightx2vで480x480の81フレーム生成可能（動作する）。
- 718: VRAMが8GBなら既に動作報告あったやろ
  - 内容: WanがVRAM8GBで動作報告あり。選ばれている理由: 動作実績あり。

### FramePack
- 該当なし。

### 追加の考察
- Wan関連の話題が圧倒的に多く、Wan2.1/2.2のGGUF量子化版（Q3, Q4, Q8など）が主に議論されており、理由としてVRAM効率、精度低下の少なさ、低スペック動作が可能などが挙げられている。これらはユーザーの環境に合わせた選択が重視されている。
- IllustriousとFLUXは少数の言及のみで、選ばれている理由はほとんど明記されていない。
- 他のモデル（NovelAI, Pony, Noobai, HiDream, FramePack）はログ内で一切触れられていない。
- もし抽出漏れや追加の文脈が必要であれば、ログの詳細を教えてください。

---以下は、提供された5chのログ（644〜844）から、生成AIの「モデル」に関する話題を抽出したものです。抽出の基準は以下の通りです：

- 生成AIのモデル（主に画像/動画生成関連の基盤モデル、LoRA、量子化版など）に関する言及を対象とし、除外モデル一覧（NovelAI v4/v3 (NAI), Pony, illustrious 0.1/1.0/1.1/2/3/3.5vpred (イラストリアス, リアス, ill), Noobai, FLUX, HiDream, Wan2.1 (wan), FramePack）に該当するものは除外しました。
- 除外リストにWan2.1 (wan)が含まれているため、Wan2.1の直接的な言及は除外しましたが、Wan2.2（リストに明示的に含まれていないため）は抽出対象と判断しました。ただし、Wan2.2の話題がWan2.1と密接に結びついている場合（例: 比較や移行の文脈）は慎重に扱い、抽出を最小限に留めました。
- 抽出はログの投稿番号を引用し、モデル名、関連する議論の内容、選ばれている理由（明示的に述べられている場合）をまとめました。重複や文脈の薄いものは統合して簡潔に。
- モデルが選ばれている理由が明確に述べられている場合、特に強調して抽出しました。全体として、Qwen-Image関連の話題が最も多く、活発な議論が見られました。

### 抽出された主なモデルと話題のまとめ

#### 1. Wan2.2（およびその量子化版、例: 14B_Q4_K_M GGUF, Q3, Q8, 14B FP8）
   - **関連投稿**: 658, 664, 665, 694, 696
   - **内容の概要**: Wan2.2のモデル選択について議論。easy版のデフォルトがGGUFのQ3であるが、Q8の方が良いか？という質問に対し、Wan2.1時代から2.2でも14B_Q4_K_MのGGUFを使っているという回答。LLMでは4bit量子化（Q4）まで精度低下が少ないと指摘。3060環境でQ8が動く例や、14B FP8がhigh/lowで28GB使用、オフロードでミドルGPU対応可能という情報。全体として、Wan2.2が「今熱い」モデルとして言及され、画像生成AIが可能なPCで動画生成が容易にできる点が評価されている。
   - **選ばれている理由**: VRAMオーバーを避けつつ精度低下を最小限に抑えられる量子化版（Q4など）が推奨されており、環境に合わせて選ぶ（例: 時間と電力の相談）。Wan2.1からの継続使用で信頼性が高い点が理由として挙げられている。

#### 2. Qwen-Image（20B MMDiTモデル, 量子化版: BF16, FP8, GGUF Q4, DF11版, MMDiT 20B + TE 2.5VL 8B）
   - **関連投稿**: 687, 688, 689, 693, 694, 702, 803, 811, 823, 826, 827, 829
   - **内容の概要**: Alibaba製の次世代T2I（Text-to-Image）モデルとして登場。20BパラメータのMMDiTで、テキストレンダリングが強く（英語でGPT-4oに匹敵、中国語でベストインクラス）。HF形式でダウンロード可能だが、BF16版は41.86GBと巨大でVRAM40GB必要。FP8版は20GB程度、GGUF Q4待ちの声。MMDiT 20B + TE 2.5VL 8Bの構成でデカすぎるとの指摘。DF11版（ロスレス圧縮量子化）が16GB VRAMで動く可能性あり、ComfyUI対応待ち。サンプル画像（ジブリ風、ジョーカー、ピカチュウ）が自重せず期待大。エロ（おっぱいなど）ほぼ無検閲の噂あり、Flux超えの高品質生成が可能。OpenAI超えの記事紹介も。
   - **選ばれている理由**: Apache2.0ライセンスでオープンソース、テキスト生成のクオリティが高い（特に中国語）。Wanシリーズの信頼からAlibaba製に期待大。エロ対応の無検閲さが魅力（Fluxのエロ制限を避けられる）。ただしスペックが高く、量子化版（DF11, GGUF Q4）待ちの理由はVRAM節約のため。追加学習の難易度が高い点がデメリットとして挙げられている。

#### 3. PapermoonのBss（モデル名不明だが、過去のモデル）
   - **関連投稿**: 763
   - **内容の概要**: モデル整理中にPapermoonニキのBssモデルを思い出し、今でも現役で最高と評価。Papermoonニキが今もモデルを上げているか質問。
   - **選ばれている理由**: 過去にいただいたものが最高で現役使用中という点から、長期的なクオリティの高さが理由。理由の詳細は不明だが、信頼性が高い模様。

### 追加の考察とフォローアップ
- **全体の傾向**: ログではWan2.2とQwen-Imageが主な焦点で、特にQwen-Imageはスペックの高さと無検閲の可能性で期待を集めています。除外モデル（例: Illustrious関連のLoRA in 713）は抽出対象外としました。Wan2.2は除外リストのWan2.1と別物として扱いましたが、もしWan一般を除外意図であれば再確認をお願いします。
- **不明点の確認**: PapermoonのBssが具体的なモデル名（例: 特定のLoRAか基盤モデルか）不明瞭ですが、モデルとして言及されているため抽出。もし詳細が必要なら、ログの文脈から追加調査可能か？
- **プロアクティブな提案**: Qwen-Imageの量子化版（DF11やGGUF）がComfyUI対応したら試用をおすすめ。VRAM節約のためオフロードの話題が多いので、関連チュートリアル（例: Hugging Faceのガイド）を共有しますか？ さらにログがあれば追加抽出します。